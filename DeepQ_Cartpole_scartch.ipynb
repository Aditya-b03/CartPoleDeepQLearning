{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqan4JG0F98f",
        "outputId": "90f14cdc-44a5-41ff-984e-a633a4157b92"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip3 install gymnasium\n",
        "!pip3 install tensorflow\n",
        "!pip3 install keras"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emVgeZRqEcA3"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "import gymnasium as gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential , load_model\n",
        "from tensorflow.keras.layers import Dense , Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmwpLCejEqRo"
      },
      "outputs": [],
      "source": [
        "#Environment\n",
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTrWuhnaErAn",
        "outputId": "7b60ab32-b02b-4b8f-984b-ac658c6c5c5f"
      },
      "outputs": [],
      "source": [
        "#Observation and action space shape\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "obs_shape = env.observation_space.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qar2x3sFRvP"
      },
      "outputs": [],
      "source": [
        "action_space = [0,1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWKsoruEGKLt"
      },
      "outputs": [],
      "source": [
        "def Model():\n",
        "  #build model\n",
        "  model = Sequential([\n",
        "  # Input_shape = (4,) [cart position, cart velocity, pole angle, pole Angular velocity]\n",
        "  Flatten(input_shape=(obs_shape,)),\n",
        "  Dense(64,activation=\"relu\"),\n",
        "  Dense(24,activation=\"relu\"),\n",
        "  Dense(len(action_space) ,activation=\"linear\")\n",
        "  ])\n",
        "  opt = Adam(learning_rate=0.01)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer = opt,\n",
        "      loss = \"mean_squared_error\"\n",
        "  )\n",
        "  return model\n",
        "\n",
        "# get new models\n",
        "model = Model()\n",
        "target_model = Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp6SaVtbLO_A"
      },
      "outputs": [],
      "source": [
        "def predict_action(state):\n",
        "\n",
        "    # returns the Q values for each corresponding action\n",
        "    action_probs = model(np.expand_dims(state, axis=0) , training=False)\n",
        "    # Take best action\n",
        "    actionIndex = tf.argmax(action_probs[0]).numpy()\n",
        "    return action_space[actionIndex]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23wAFpi9LthW"
      },
      "outputs": [],
      "source": [
        "# training parameter\n",
        "MEMORY_SIZE     = 50000\n",
        "GAMMA          = 0.90 # Discount factor for future rewards\n",
        "EPSILON_MAX     = 1.0  # Initial exploration rate\n",
        "EPSILON_MIN    = 0.1\n",
        "EPSILON_DECAY   = 0.0003\n",
        "BATCH_SIZE      = 32   # Size of batch taken from replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-kURjjnBD_U"
      },
      "outputs": [],
      "source": [
        "# function updates/fits the model to simulate updates in Q - values\n",
        "def update_q_vals(model , replay):\n",
        "  # Take random samples from the memory\n",
        "  samples = random.sample(replay, BATCH_SIZE)\n",
        "  \n",
        "  input_state = []          # stores current sample states - think of it as training X/Input to the model\n",
        "  target_q_values = []      # stores updated Q - values - think of it as training Y/required_outputs from the model\n",
        "\n",
        "\n",
        "  for state, action, next_state, reward, done_sample in samples:\n",
        "      # get corresponding Q-value of the state\n",
        "      current_q_value = model(np.expand_dims(state, axis=0) , training=False).numpy()\n",
        "      current_q_value = current_q_value[0]\n",
        "\n",
        "      # update the Q-value accordingly\n",
        "      if done_sample:\n",
        "          #print(current_q_value[action])\n",
        "          current_q_value[action] = reward\n",
        "      else:\n",
        "          future_reward = target_model(np.expand_dims(next_state, axis=0) , training=False).numpy()\n",
        "          #print(future_reward[0][tf.argmax(future_reward[0]).numpy()])\n",
        "          current_q_value[action] = reward + GAMMA *future_reward[0][tf.argmax(future_reward[0]).numpy()]\n",
        "\n",
        "      # append to the input X and output Y\n",
        "      input_state.append(state)\n",
        "      target_q_values.append(current_q_value)\n",
        "  \n",
        "  # fit the model to correctly predict Ys for corresponding Xs\n",
        "  model.fit(np.array(input_state), np.array(target_q_values), epochs=1, verbose=0)\n",
        "\n",
        "  return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOKCK3LsMHcZ"
      },
      "outputs": [],
      "source": [
        "def train(model=model,\n",
        "          target_model=target_model,\n",
        "          starting_state=0,\n",
        "          epsilon=EPSILON_MAX,\n",
        "          decay_rate = EPSILON_DECAY,\n",
        "          starting_episode = 0,\n",
        "          save_model=50,\n",
        "          render = False,\n",
        "          memory_size = MEMORY_SIZE,\n",
        "          MAX_EPISODE = 300\n",
        "          ):\n",
        "  \n",
        "  # output file\n",
        "  file = open(f\"outputs/output_{starting_episode}.txt\" , \"w\")\n",
        "  file.write(\"Start training\\n\")\n",
        "  \n",
        "  #Experience replay buffers - Memory\n",
        "  replay = deque(maxlen = memory_size)\n",
        "\n",
        "  # number of steps taken already - default 0\n",
        "  state_count = starting_state\n",
        "\n",
        "  # keeps track of final rewards for last 100 episodes\n",
        "  episode_reward_history = []\n",
        "  \n",
        "\n",
        "  epsilon_decay = decay_rate\n",
        "  \n",
        "  update_after_actions = 4 # update q values after n steps \n",
        "  update_target_network = 20 # updaete target model after n episodes\n",
        "  \n",
        "  running_reward = 0\n",
        "  episode_count = starting_episode\n",
        "\n",
        "\n",
        "  for episode in range(episode_count , MAX_EPISODE):\n",
        "    # initial state\n",
        "    state = np.array(env.reset()[0])\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "\n",
        "\n",
        "    while not done: #run until episode is finished\n",
        "      \n",
        "      # increase steps taken count\n",
        "      state_count+=1\n",
        "\n",
        "      if render:\n",
        "        env.render()\n",
        "\n",
        "      # predict action\n",
        "      if np.random.rand() < epsilon :   # randomly\n",
        "        action = random.choice(action_space)\n",
        "\n",
        "      else :                            # using model\n",
        "        action = predict_action(state)\n",
        "\n",
        "\n",
        "      # take the action\n",
        "      next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "      # extra reward for pole Angle being in ±6˚\n",
        "      if(-0.1048 <= next_state[2] <= 0.1048):\n",
        "        reward += 0.2*reward\n",
        "      episode_reward+=reward\n",
        "\n",
        "      # Decay probability of taking random action\n",
        "      epsilon -= epsilon_decay\n",
        "      epsilon = max(epsilon, EPSILON_MIN)\n",
        "\n",
        "      # update the memory\n",
        "      replay.append((state, action, next_state, reward, done))\n",
        "\n",
        "      # preform replay/update model with new Q values\n",
        "      if state_count % update_after_actions == 0 and len(replay) > BATCH_SIZE:\n",
        "        model = update_q_vals(model , replay)\n",
        "      \n",
        "\n",
        "      state = next_state\n",
        "    \n",
        "    #print the final episode reward\n",
        "    file.write(f\"episode_{episode} : {episode_reward}\\n\")\n",
        "    file.flush()\n",
        "\n",
        "    #update reward history\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if len(episode_reward_history) > 100:\n",
        "        del episode_reward_history[:1]\n",
        "\n",
        "    # running reward = mean over last 100 episodes\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "    \n",
        "    # update the the target network with new weights\n",
        "    if episode%update_target_network == 0:\n",
        "      target_model.set_weights(model.get_weights())\n",
        "      # Log details\n",
        "      template = \"running reward: {:.2f} at episode {}, state count {}\\n\"\n",
        "      file.write(template.format(running_reward, episode, state_count))\n",
        "\n",
        "    # save model\n",
        "    if episode%save_model == 0:\n",
        "      file.write(f\"epsilon = {epsilon} , state_count = {state_count} , episode_count = {episode_count}\\n\")\n",
        "      model.save('models/trained_{}.h5'.format(episode))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc4BgzDee-0W",
        "outputId": "101b29c1-9ded-4085-ddc8-e770afddc0d3"
      },
      "outputs": [],
      "source": [
        "loaded_model = False\n",
        "# models/trained_600.h5\n",
        "load_model_path = \"\"\n",
        "\n",
        "# load a saved model and train\n",
        "if loaded_model:\n",
        "  model = load_model(load_model_path)\n",
        "  target_model = load_model(load_model_path)\n",
        "  epsi = 1  # epsilon\n",
        "  sc = 0    # state_count\n",
        "  epc = 0   # episode_count\n",
        "  model = train(model = model , target_model = target_model , epsilon=epsi , starting_state = sc , starting_episode = epc , MAX_EPISODE= epc+300)\n",
        "\n",
        "# train new model\n",
        "else:\n",
        "  model = train(model = model , target_model = target_model ,MAX_EPISODE = 300)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
